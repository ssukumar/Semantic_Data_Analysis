---
title: "analyze_semantic_data"
output: html_document
date: "2024-03-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# set the directory (need to make it more general)
# setwd('Documents/Postdoc/Semantic_Fluency/Semantic_Data_Analysis/PILOT')

Online_data = TRUE # IF true, then data is collected on prolific, else in person data

if (Online_data){
  data_dir = 'DATA/Corrected/Grouped_corrected'
  plots_dir = 'Plots_corrected'
} else {
  data_dir = 'DATA/In_Person/Grouped'
  plots_dir = 'Plots_in_person'
}
forager_suffix = "_outputs_forager_results"
library(ggplot2)
library(dplyr)
```

## Plot the semantic similarity and the 

Read in the relevant .csv file for analysis

```{r import_file}
category = 'BODY PARTS'
category_dir = file.path(data_dir, paste0(category, forager_suffix))
lexical_fn= 'lexical_results.csv'

lexical_file = file.path(category_dir, lexical_fn)

lexical_data = read.csv(lexical_file)

lexical_data = lexical_data %>% group_by(Subject) %>% mutate(Item = row_number())

```

## Plot Semantic Similarity and Frequency by subject

First, plot frequency
```{r freq_plot, echo=FALSE}

plt_freq_subj =  ggplot(data = lexical_data) + geom_line(aes( x = Item, y = Frequency_Value, color = Subject, group = Subject))+
  facet_wrap(~Subject, nrow = 4, scales = "free")+ 
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))

plt_freq_subj
ggsave(file.path(plots_dir, paste0(category, '_freqency_subj.png')), plt_freq_subj, units = "in", width = 8, height = 5)
  
```

Aggregate Data and check

```{r}

lex_avg_subj = lexical_data %>% group_by(Item) %>% summarise(mean_freq = mean(Frequency_Value), sd_freq = sd(Frequency_Value), mean_ss= mean(Semantic_Similarity), sd_ss = sd(Semantic_Similarity))

lex_avg_subj = lex_avg_subj %>% mutate(freq_min = mean_freq - sd_freq, freq_max = mean_freq + sd_freq, ss_min = mean_ss - sd_ss, ss_max = mean_ss + sd_ss)

ggplot(data=lex_avg_subj) + geom_line(aes(x = Item, y = mean_freq))+
  geom_errorbar(aes(x = Item, ymin = freq_min, ymax = freq_max))+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))



lex_filter_outliers = lexical_data %>% group_by(Item) %>% filter((Frequency_Value < 2 *sd(Frequency_Value) + mean(Frequency_Value)))

lex_avg_subj_outl_freq = lex_filter_outliers %>% group_by(Item) %>% summarise(mean_freq = mean(Frequency_Value), sd_freq = sd(Frequency_Value))

lex_avg_subj_outl_freq = lex_avg_subj_outl_freq %>% mutate(freq_min = mean_freq - sd_freq, freq_max = mean_freq + sd_freq)

plt_avg_freq = ggplot(data=lex_avg_subj_outl_freq) + geom_line(aes(x = Item, y = mean_freq))+
  geom_errorbar(aes(x = Item, ymin = freq_min, ymax = freq_max))+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
plt_avg_freq
ggsave(file.path(plots_dir ,paste0(category, '_freqency_avg.png')), plt_avg_freq, units = "in", width = 8, height = 5)

```

Next, semantic similarity 
```{r freq_plot, echo=FALSE}

plt_ss_subj = ggplot(data = lexical_data) + geom_line(aes( x = Item, y = Semantic_Similarity, color = Subject, group = Subject))+
  facet_wrap(~Subject, nrow = 4, scales = "free_x")+ 
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
  
plt_ss_subj
ggsave(file.path(plots_dir, paste0(category, '_semanticsim_subj.png')), plt_ss_subj, units = "in", width = 8, height = 5)

```
Aggregate the data

```{r}
 ggplot(data=lex_avg_subj) + geom_line(aes(x = Item, y = mean_ss))+
  geom_errorbar(aes(x = Item, ymin = ss_min, ymax = ss_max))+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))



lex_filter_outliers = lexical_data %>% group_by(Item) %>% filter((Semantic_Similarity < 2 *sd(Semantic_Similarity) + mean(Semantic_Similarity)))

lex_avg_subj_outl_ss = lex_filter_outliers %>% group_by(Item) %>% summarise(mean_ss = mean(Semantic_Similarity), sd_ss = sd(Semantic_Similarity))

lex_avg_subj_outl_ss = lex_avg_subj_outl_ss %>% mutate(ss_min = mean_ss - sd_ss, ss_max = mean_ss + sd_ss)

plt_ss_avg =ggplot(data=lex_avg_subj_outl_ss) + geom_line(aes(x = Item, y = mean_ss))+
  geom_errorbar(aes(x = Item, ymin = ss_min, ymax = ss_max))+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
plt_ss_avg
ggsave(file.path(plots_dir,paste0(category, '_semanticsim_avg.png')), plt_ss_avg, units = "in", width = 8, height = 5)

```


# Raw timing outputs

Data in the above plots all look at processed data from the frequency computations of `forager`. However, this will have a lot of excluded data and may not be representative of how often people are talking. While we're waiting for data to be processed to minimize the exclusions, it may be worth looking at the timings in the raw transcribed outputs to jusst get at how many silences there are v/s chatters. 


```{r}


raw_file_suffix = '_outputs.csv'
raw_output_file = file.path(data_dir, paste0(category, raw_file_suffix))

raw_output_df = read.table(raw_output_file, quote = "",  header = TRUE, sep = ",")
raw_output_df$End_time = as.double(raw_output_df$End_time)
raw_output_df = unique(raw_output_df)
raw_output_df = raw_output_df %>% group_by(subj_id) %>% mutate(Item = row_number(), IRT = Start_time - lag(End_time), 
                                                               Word = tolower(Word))

# exclude the one low response subject

# raw_output_df = raw_output_df[raw_output_df$subj_id != '5f1f3581b5cf3e103cc545c9',]

ggplot(data = raw_output_df) + geom_line(aes( x = Item, y = IRT, color = subj_id, group = subj_id))+
  facet_wrap(~subj_id, nrow = 5, scales = "free")+ 
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))

# normalize and plot interitem time

raw_output_df = raw_output_df %>% group_by(subj_id) %>% mutate(norm_IRT = IRT/ mean(IRT, na.rm = TRUE))


subj_norm_IRT_plt =ggplot(data = raw_output_df) + geom_line(aes( x = Item, y = norm_IRT, group = subj_id))+
  facet_wrap(~subj_id, nrow = 5, scales = "free")+ 
  geom_hline(yintercept = 1, col = "blue", linetype = "dashed")+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))

ggsave(file.path(plots_dir,paste0(category, '_subj_norm_IRT_plt.png')), subj_norm_IRT_plt, units = "in", width = 8, height = 5)
```
## Merge dataframes 

```{r}
names(raw_output_df)[1:2] = c("Subject", "Fluency_Item")
check_merge = merge(lexical_data, raw_output_df, by = c("Subject", "Fluency_Item"))


```


## average normalized interitem time raw 

```{r}

raw_output_subj = raw_output_df %>% group_by(Item) %>% summarise(mean_IRT = mean(IRT), sd_IRT = sd(IRT), mean_norm_IRT= mean(norm_IRT), sd_norm_IRT = sd(norm_IRT))
raw_output_subj = raw_output_subj %>% mutate(IRT_min = mean_IRT - sd_IRT, IRT_max = mean_IRT + sd_IRT, norm_IRT_min = mean_norm_IRT - sd_norm_IRT, norm_IRT_max = mean_norm_IRT + sd_norm_IRT)

# ggplot(data=raw_output_subj) + geom_line(aes(x = Item, y = mean_IRT))+
#   geom_errorbar(aes(x = Item, ymin = IRT_min, ymax = IRT_max))+
#   ggtitle(category)+
#   theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
# 
# ggplot(data=raw_output_subj) + geom_line(aes(x = Item, y = mean_norm_IRT))+
#   geom_errorbar(aes(x = Item, ymin = norm_IRT_min, ymax = norm_IRT_max))+
#   ggtitle(category)+
#   theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))

raw_filter_outliers = raw_output_df %>% group_by(Item) %>% filter((IRT < 2 *sd(IRT) + mean(IRT)),
                                                                  (norm_IRT < 2 *sd(norm_IRT) + mean(norm_IRT)))

raw_avg_subj_outl = raw_filter_outliers %>% group_by(Item) %>% summarise(mean_IRT = mean(IRT), sd_IRT = sd(IRT), 
                                                                         mean_norm_IRT = mean(norm_IRT), sd_norm_IRT = sd(norm_IRT))

raw_avg_subj_outl = raw_avg_subj_outl %>% mutate(IRT_min = mean_IRT - sd_IRT, IRT_max = mean_IRT + sd_IRT,
                                          norm_IRT_min = mean_norm_IRT - sd_norm_IRT, norm_IRT_max = mean_norm_IRT + sd_norm_IRT)

avg_IRT_plt = ggplot(data=raw_avg_subj_outl) + geom_line(aes(x = Item, y = mean_IRT))+
  geom_errorbar(aes(x = Item, ymin = IRT_min, ymax = IRT_max))+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
avg_IRT_plt
ggsave(file.path(plots_dir,paste0(category, 'avg_IRT_plt.png') ), avg_IRT_plt, units = "in", width = 8, height = 5)

avg_norm_IRT_plt = ggplot(data=raw_avg_subj_outl) + geom_line(aes(x = Item, y = mean_norm_IRT))+
  geom_errorbar(aes(x = Item, ymin = norm_IRT_min, ymax = norm_IRT_max))+
  geom_hline(yintercept = 1, col = "blue", linetype = "dashed")+
  ggtitle(category)+
  theme(legend.position="none",plot.title = element_text(size=22, hjust = 0.5))
avg_norm_IRT_plt
ggsave(file.path(plots_dir,paste0(category, '_avg_norm_IRT_plt.png')), avg_norm_IRT_plt, units = "in", width = 8, height = 5)

```
# Subplot of IRT, frequency and semantic similarity metrics

```{r}

aggregate_metrics_df = merge(lex_avg_subj_outl_freq, lex_avg_subj_outl_ss, by = "Item")
aggregate_metrics_df = merge(aggregate_metrics_df, raw_avg_subj_outl, by ="Item")

plt_irt = ggplot(data= aggregate_metrics_df) + geom_line(aes(x = Item, y = mean_norm_IRT))+
  geom_errorbar(aes(x = Item, ymin = norm_IRT_min, ymax = norm_IRT_max))

# plt_irt = plt_irt + geom_line(aes(x = Item, y = mean_freq))+
#   geom_errorbar(aes(x = Item, ymin = freq_min, ymax = freq_max))

# plt_irt + geom_line(aes(x = Item, y = mean_ss))+
#   geom_errorbar(aes(x = Item, ymin =ss_min, ymax = ss_max))


```


# Identify excluded words for each subject between the processed and unprocessed data


Focusing on a single category for now 


```{r}
library(fuzzyjoin)
library(stringdist)
colnames(raw_output_df)[1] = "Subject"
colnames(raw_output_df)[2] = "Fluency_Item"

is_name_distance_below_four <- function(left, right) {
  stringdist(left, right) < 4
}


```


Across all categories, find the average number of words uttered 

```{r}

# Group A Categories

categoriesA = c('ANIMALS','BODY PARTS', 'FRUITS', 'HOBBIES', 'HOLIDAYS', 'HOME APPLIANCES', 'LIQUIDS', 'MEANS OF TRANSPORT', 'MEDICAL CONDITIONS', 'MUSICAL INSTRUMENTS', 'OCCUPATIONS', 'TOOLS', 'VEGETABLES')

categoriesB = c('ACCOMODATIONS', 'CLOTHING', 'COUNTRIES', 'FLOWERS', 'FOODS', 'FURNITURE', 'NATURAL ENVIRONMENT', 'NON-FOOD ITEMS FOUND IN SUPERMARKET', 'OFFICE SUPPLIES', 'SPORTS', 'TECHNOLOGY GADGETS & DEVICES', 'THINGS FOUND IN KITCHENS', 'TOYS')

categories = c(categoriesA, categoriesB)
# categories = categoriesA
# print(categories)
desc_stats_fn = "individual_descriptive_stats.csv"

num_items_df = data.frame()
for (category in categories){
  category_dir = file.path(data_dir, paste0(category, forager_suffix))
  desc_stats_file = file.path(category_dir, desc_stats_fn)
  desc_df = read.csv(desc_stats_file)
  desc_df = desc_df %>% mutate(Category = category, Group = ifelse(category %in% categoriesA, 'A', 'B') )
  if (ncol(desc_df) == 9){
    desc_df = desc_df %>% select(-c(Phonological_Similarity_mean, Phonological_Similarity_std))
  }
  num_items_df = rbind(num_items_df, desc_df)
}
print(num_items_df)
num_items_cat = num_items_df %>% group_by(Category) %>% summarise( mean_num_items = mean(X._of_Items, na.rm = TRUE), sd_num_items = sd(X._of_Items, na.rm = TRUE))
num_items_cat = num_items_cat%>% mutate(num_items_min = mean_num_items - sd_num_items, num_items_max = mean_num_items + sd_num_items)

plt_category_words = ggplot(data= num_items_cat) + geom_bar(aes(x= reorder(Category, -mean_num_items),  y = mean_num_items, fill = Category), stat='identity')+
  geom_errorbar(aes(x= Category, ymin = num_items_min, ymax= num_items_max), width = 0)+
  xlab('Category')+
  theme(legend.position="none", axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
plt_category_words
ggsave(file.path(plots_dir,'plt_category_words_avg.png'), plt_category_words, units = "in", width = 12, height = 6)


# num_items_subj = num_items_df %>% group_by(Subject) %>% summarise( mean_num_items = mean(X._of_Items), sd_num_items = sd(X._of_Items))
```

Check performance by subject


```{r}

# abbreviate category names

num_items_dfA = num_items_df[num_items_df$Group=='A',]
plt_category_words_subj = ggplot(data= num_items_dfA) + geom_bar(aes(x= Category,  y = X._of_Items, fill = Category), stat='identity')+
  facet_wrap(~Subject, nrow= 3)+
  scale_x_discrete(label=function(x) stringr::str_trunc(x, 12))+
  theme(legend.position="none", axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
plt_category_words_subj
ggsave(file.path(plots_dir,'plt_category_words_subjA.png'), plt_category_words_subj, units = "in", width = 12, height = 8)

num_items_dfB = num_items_df[num_items_df$Group=='B',]
plt_category_words_subj = ggplot(data= num_items_dfB) + geom_bar(aes(x= Category,  y = X._of_Items, fill = Category), stat='identity')+
  facet_wrap(~Subject, nrow= 2)+
  scale_x_discrete(label=function(x) stringr::str_trunc(x, 12))+
  theme(legend.position="none", axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
plt_category_words_subj
ggsave(file.path(plots_dir,'plt_category_words_subjB.png'), plt_category_words_subj, units = "in", width = 12, height = 8)


```



Attempt fuzzy join of data sources 

```{r}
df_Exclude = stringdist_anti_join(raw_output_df, lexical_data, by = c("Subject", "Fluency_Item"), max_dist = 3, method='lv')
```

Function to determine mean number of responses
```{r}
num_items_desc <- function(category_list, data_dir, desc_stats_fn, forager_suffix){
  num_items_df = data.frame()
  for (category in category_list){
    category_dir = file.path(data_dir, paste0(category, forager_suffix))
    desc_stats_file = file.path(category_dir, desc_stats_fn)
    desc_df = read.csv(desc_stats_file)
    desc_df = desc_df %>% mutate(Category = category, Group = ifelse(category %in% categoriesA, 'A', 'B') )
    if (ncol(desc_df) == 9){
      desc_df = desc_df %>% select(-c(Phonological_Similarity_mean, Phonological_Similarity_std))
    }
    num_items_df = rbind(num_items_df, desc_df)
  }
  
  num_items_cat = num_items_df %>% group_by(Category) %>% summarise( mean_num_items = mean(X._of_Items, na.rm = TRUE), sd_num_items = sd(X._of_Items, na.rm = TRUE))
  num_items_cat = num_items_cat%>% mutate(num_items_min = mean_num_items - sd_num_items, num_items_max = mean_num_items + sd_num_items, Group = ifelse(Category %in% categoriesA, 'A', 'B'))
  
  return(list(num_items_cat, num_items_df))
}
```



Compare in person and online number of items 

```{r}
data_online_dir = 'DATA/Corrected/Grouped_corrected'
data_inperson_dir = 'DATA/In_Person/Grouped'
forager_suffix = "_outputs_forager_results"
categoriesA = c('ANIMALS','BODY PARTS', 'FRUITS', 'HOBBIES', 'HOLIDAYS', 'HOME APPLIANCES', 'LIQUIDS', 'MEANS OF TRANSPORT', 'MEDICAL CONDITIONS', 'MUSICAL INSTRUMENTS', 'OCCUPATIONS', 'TOOLS', 'VEGETABLES')

categoriesB = c('ACCOMODATIONS', 'CLOTHING', 'COUNTRIES', 'FLOWERS', 'FOODS', 'FURNITURE', 'NATURAL ENVIRONMENT', 'NON-FOOD ITEMS FOUND IN SUPERMARKET', 'OFFICE SUPPLIES', 'SPORTS', 'TECHNOLOGY GADGETS & DEVICES', 'THINGS FOUND IN KITCHENS', 'TOYS')

# categories = c(categoriesA, categoriesB)
categorues
# print(categories)
desc_stats_fn = "individual_descriptive_stats.csv"

num_items_cat_online_list = num_items_desc(categories, data_online_dir, desc_stats_fn, forager_suffix)
num_items_cat_online = num_items_cat_online_list[[1]]
num_items_cat_online = num_items_cat_online %>% mutate(data_type = 'Online')
num_items_desc_online = num_items_cat_online_list[[2]]
num_items_desc_online = num_items_desc_online %>% mutate(data_type = 'Online')

num_items_cat_inperson_list = num_items_desc(categories, data_inperson_dir, desc_stats_fn, forager_suffix)
num_items_cat_inperson = num_items_cat_inperson_list[[1]]
num_items_cat_inperson = num_items_cat_inperson %>% mutate(data_type = 'In_Person')
num_items_desc_inperson = num_items_cat_inperson_list[[2]]
num_items_desc_inperson = num_items_desc_inperson %>% mutate(data_type = 'In_Person')

num_items_cat_all = rbind(num_items_cat_online, num_items_cat_inperson)
num_items_desc_all = rbind(num_items_desc_online, num_items_desc_inperson)

```

Plot comparisons between onine and in person data

```{r}

num_items_catA = num_items_cat_all[num_items_cat_all$Group == 'A',]

plt_compare_A = ggplot(data= num_items_catA , aes(x= reorder(Category, -mean_num_items), y = mean_num_items, fill = data_type, ymin = num_items_min, ymax= num_items_max)) + 
   geom_bar( stat='identity', position=position_dodge(0.9))+
   geom_errorbar(width = 0, position=position_dodge(0.9))+
   xlab('Category')+ggtitle('Category A')+
   theme(axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
 
plt_compare_A

 ggsave(file.path(plots_dir,'plt_compareA.png'), plt_compare_A, units = "in", width = 12, height = 8)
 
 num_items_catB = num_items_cat_all[num_items_cat_all$Group == 'B',]

plt_compare_B = ggplot(data= num_items_catB, aes(x= reorder(Category, -mean_num_items),  y = mean_num_items, fill = data_type, ymin = num_items_min, ymax= num_items_max)) + 
   geom_bar(stat='identity', position=position_dodge(0.9))+
   geom_errorbar(width = 0, position=position_dodge(0.9))+
   xlab('Category')+ggtitle('Category B')+
   theme(axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
plt_compare_B
 ggsave(file.path(plots_dir,'plt_compareB.png'), plt_compare_B, units = "in", width = 12, height = 8)

```
# statistical testing to analyze the effect of online v/s in person experimentation

```{r}
num_cat_aggregate = num_items_cat_all %>% group_by(data_type) %>% summarise( mean_all_items = mean(mean_num_items, na.rm = TRUE), sd_all_items = sd(mean_num_items))

violin_plt = ggplot(data= num_items_cat_all, aes(x = data_type, y = mean_num_items)) + geom_violin(trim = FALSE)+
  stat_summary(fun.data=mean_sdl, mult=1, 
                 geom="pointrange", color="red")
 ggsave(file.path(plots_dir,'plt_compareB.png'), violin_plt, units = "in", width = 12, height = 8)

```
# Category-wise analysis of variation 

```{r}
p_list = list()
test_df = data.frame()
for(category in categories){
  num_cat = num_items_desc_all[num_items_desc_all$Category == category,]
  num_online = subset(num_cat, data_type == "Online")
  num_in_person = subset(num_cat, data_type == "In_Person")
  
  num_test = t.test(num_online$X._of_Items, num_in_person$X._of_Items, var.equal = FALSE)
  
  if (length(p_list) == 0){
    p_list = num_test$p.value
  }else{
    p_list = c(p_list, num_test$p.value)
  }
  
  test_df = rbind(test_df, c(category, num_test$statistic, num_test$p.value))
}

colnames(test_df)= c('Category', 'T_Value', 'P_Value')
test_df$P_Value = as.numeric(test_df$P_Value)
test_df$T_Value = as.numeric(test_df$T_Value)
```


```{r}
ggplot(data= test_df, aes(x = Category)) + 
  stat_summary(aes(y = P_Value), fun = "mean", geom = "bar")+
  geom_hline(yintercept = 0.05)+
   theme(axis.text.x = element_text(angle = 45, vjust =1, hjust=1))

ggplot(data= test_df, aes(x = Category)) + 
  stat_summary(aes(y = T_Value), fun = "mean", geom = "bar")+
   theme(axis.text.x = element_text(angle = 45, vjust =1, hjust=1))
   
```


# plot switches in data 

First, determine different cluster sizes for different individuals
Avg cluster size per category

Avg time spent per cluster per category

```{r}
switch_counts = data.frame(matrix(ncol = 3, nrow = 0))

```

```{r}

# determine cluster size for each subject
model_dir = 'DATA/Grouped_Models'
model_file = 'model_results.csv'
switch_file = 'switch_results.csv'


for (category in categories){
  category
  category_dir = file.path(model_dir, paste0(category, forager_suffix))
  switch_df = read.csv(file.path(category_dir, switch_file))
  clust_size = 0
  for(i in 1:nrow(switch_df)){
    subj = switch_df[i, "Subject"]
    if (switch_df[i, 'Switch_Value']==0 || switch_df[i, 'Switch_Value']==2){
      clust_size = clust_size + 1
    } else {
      switch_counts = rbind(switch_counts, c(subj, clust_size, category))
      clust_size = 0
    }
    
  }
}
colnames(switch_counts) = c('Subject', 'cluster_size', 'Category')
switch_counts$cluster_size = as.numeric(switch_counts$cluster_size)
```


## Average across subejct

```{r}

agg_switch_sum = switch_counts %>% group_by(Category, Subject) %>% summarise(cluster_size = mean(cluster_size))

ggplot(data = agg_switch_sum) + geom_point(aes(x= Category, y= cluster_size, color =Category))+
  theme(legend.position="none",axis.text.x = element_text(angle = 45, vjust =1, hjust=1))

```


# Analyze switches in individual categories 

```{r}
# merge lexical and switch datasets 
category = 'FRUITS'
category_dir = file.path(data_dir, paste0(category, forager_suffix))
lexical_fn= 'lexical_results.csv'

lexical_file = file.path(category_dir, lexical_fn)

lexical_data = read.csv(lexical_file)
switch_df = read.csv(file.path(category_dir, switch_file))
lex_switch_df = merge(lexical_data, switch_df, by = c('Subject', 'Fluency_Item'))



```

